{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd5ee1d",
   "metadata": {},
   "source": [
    "# Chatbot with message summarization\n",
    "## Review\n",
    "We've covered how to customize graph state schema and reducer.\n",
    "\n",
    "We've also shown a number of ways to trim or filter messages in graph state.\n",
    "\n",
    "## Goals\n",
    "Now, let's take it one step further!\n",
    "\n",
    "Rather than just trimming or filtering messages, we'll show how to use LLMs to produce a running summary of the conversation.\n",
    "\n",
    "This allows us to retain a compressed representation of the full conversation, rather than just removing it with trimming or filtering.\n",
    "\n",
    "We'll incorporate this summarization into a simple Chatbot.\n",
    "\n",
    "And we'll equip that Chatbot with memory, supporting long-running conversations without incurring high token cost / latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf2e150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env_(var):\n",
    "    if not os.environ.get(var):\n",
    "        print(\"--- API KEY NOT PRESENT ---\")\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "_set_env_(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5c7829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env_(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "854d8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"qwen-qwq-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cefa28c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user said \"Hello\". I need to respond in a friendly way. Let me think of a simple greeting. Maybe \"Hi there!\" and ask how I can assist them. Keep it open-ended so they feel comfortable to ask anything. I should make sure the tone is warm and approachable. Yeah, that should work.\\n\\nWait, should I use an emoji? The example response has a waving hand. Maybe add that to be friendly. So, \"Hi there! ðŸ‘‹ How can I assist you today?\" That sounds good. It\\'s concise and invites them to share their needs. I don\\'t want to overcomplicate it. Let\\'s go with that.\\n</think>\\n\\nHi there! ðŸ‘‹ How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 11, 'total_tokens': 163, 'completion_time': 0.378157579, 'prompt_time': 0.002921622, 'queue_time': 0.45432022699999997, 'total_time': 0.381079201}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_3796682456', 'finish_reason': 'stop', 'logprobs': None}, id='run--974de86d-00b5-4408-bea5-2a873237edc3-0', usage_metadata={'input_tokens': 11, 'output_tokens': 152, 'total_tokens': 163})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd1876c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class State(MessagesState):\n",
    "    summery: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9af45d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "import json\n",
    "\n",
    "def chat_model(state: State) -> json:\n",
    "    \n",
    "    summery = state.get(\"summery\")\n",
    "\n",
    "    if summery:\n",
    "        system_message = f\"Summery of the earlier conversation: {summery}\"\n",
    "\n",
    "        messages = [SystemMessage(content=system_message) + state[\"messages\"]]\n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "\n",
    "    return {\"messages\": llm.invoke(messages)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3afd615",
   "metadata": {},
   "source": [
    "We'll define a node to produce a summary.\n",
    "\n",
    "Note, here we'll use `RemoveMessage` to filter our state after we've produced the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f66cdb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summerize_conversation(state: State): \n",
    "    summery = state.get(\"summery\", \"\")\n",
    "\n",
    "    if summery:\n",
    "\n",
    "        summery_message=(\n",
    "            f\"This is summery of the conversation: {summery} \\n \\n\"\n",
    "            \"Extend the summery by taking into account the new messages above:\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        summery_message= \"Create a summery of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summery_message)]\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Delete all but the last two messages\n",
    "    # to keep the context small\n",
    "    delete_message = [RemoveMessage(m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summery\": response.content, \"messages\": delete_message }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b111ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def should_continue(state: State):\n",
    "    messages= state[\"messages\"]\n",
    "    if len(messages) > 5:\n",
    "        return \"summrize conversation\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "257a05bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found edge starting at unknown node 'summarize_conversation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Compile\u001b[39;00m\n\u001b[32m     16\u001b[39m memory = MemorySaver()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m graph = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m display(Image(graph.get_graph().draw_mermaid_png()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kunal\\anaconda3\\envs\\LangGraph\\Lib\\site-packages\\langgraph\\graph\\state.py:592\u001b[39m, in \u001b[36mStateGraph.compile\u001b[39m\u001b[34m(self, checkpointer, store, interrupt_before, interrupt_after, debug, name)\u001b[39m\n\u001b[32m    589\u001b[39m interrupt_after = interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    591\u001b[39m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    596\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[32m    601\u001b[39m output_channels = (\n\u001b[32m    602\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__root__\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    603\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.schemas[\u001b[38;5;28mself\u001b[39m.output]) == \u001b[32m1\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    609\u001b[39m     ]\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kunal\\anaconda3\\envs\\LangGraph\\Lib\\site-packages\\langgraph\\graph\\graph.py:272\u001b[39m, in \u001b[36mGraph.validate\u001b[39m\u001b[34m(self, interrupt)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m all_sources:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nodes \u001b[38;5;129;01mand\u001b[39;00m source != START:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound edge starting at unknown node \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m START \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_sources:\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    276\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGraph must have an entrypoint: add at least one edge from START to another node\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    277\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found edge starting at unknown node 'summarize_conversation'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", chat_model)\n",
    "workflow.add_node(summerize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d324a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangGraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
